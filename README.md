# ğŸ›’ Amazon Reviews Scraper

Un scraper d'avis Amazon robuste et configurable avec rÃ©silience, rotation de proxies, et export multi-format.

[![CI/CD](https://github.com/your-username/amazon-scraper/workflows/CI/CD%20Pipeline/badge.svg)](https://github.com/your-username/amazon-scraper/actions)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

## âš ï¸ Avertissement de conformitÃ©

**IMPORTANT** : Le scraping peut contrevenir aux Conditions GÃ©nÃ©rales d'Utilisation d'Amazon. Ce projet est fourni Ã  des fins Ã©ducatives et de recherche uniquement. L'utilisateur est responsable de l'utilisation conforme et lÃ©gale de cet outil.

### Bonnes pratiques recommandÃ©es :
- Respectez une cadence raisonnable (2-4 secondes entre les requÃªtes)
- Limitez le nombre de pages par ASIN (5-10 maximum)
- Utilisez des proxies rotatifs pour Ã©viter la dÃ©tection
- Ne collectez pas de donnÃ©es personnelles non nÃ©cessaires
- Respectez les robots.txt et les CGU d'Amazon

## ğŸš€ FonctionnalitÃ©s

- **Scraping robuste** : Gestion des erreurs, retry automatique, dÃ©tection anti-bot
- **Rotation de proxies** : Support de pools de proxies pour Ã©viter la dÃ©tection
- **Pagination robuste** : Next + fallback URL si nÃ©cessaire, scrape jusquâ€™Ã  la fin
- **Base de donnÃ©es** : Stockage SQLite (dÃ©faut) ou PostgreSQL
- **Export multi-format** : CSV, Parquet, NDJSON
- **CLI ergonomique** : Interface en ligne de commande intuitive
- **Tests complets** : Couverture de tests >90%
- **Docker ready** : Conteneurisation complÃ¨te
- **CI/CD** : Pipeline d'intÃ©gration continue

## ğŸ“‹ PrÃ©requis

- Python 3.11+
- Playwright (navigateurs installÃ©s automatiquement)
- Base de donnÃ©es SQLite (incluse) ou PostgreSQL (optionnel)

## ğŸ› ï¸ Installation

### Installation locale

1. **Cloner le repository**
```bash
git clone https://github.com/your-username/amazon-scraper.git
cd amazon-scraper
```

2. **CrÃ©er l'environnement virtuel**
```bash
make venv
# ou
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

3. **Installer les dÃ©pendances**
```bash
make dev
# ou
pip install -r requirements.txt
python -m playwright install chromium
```

4. **Configuration**
```bash
cp env.example .env
# Ã‰diter .env selon vos besoins
```

### Installation avec Docker

1. **Construction de l'image**
```bash
make docker-build
# ou
docker build -t amazon-scraper .
```

2. **Lancement avec docker-compose (recommandÃ©)**
```bash
make docker-compose-up
# ou
docker-compose up -d
```

## âš™ï¸ Configuration

### Variables d'environnement (.env)

```bash
# Proxies (optionnel)
PROXY_POOL=http://user:pass@proxy1:8000,http://user:pass@proxy2:8000

# Navigateur
HEADLESS=true
TIMEOUT_MS=45000
MAX_CONTEXTS=2

# Scraping
MAX_PAGES_PER_ASIN=5
SLEEP_MIN=2.0
SLEEP_MAX=4.0
LANGUAGE=fr_FR
SORT=recent

# Base de donnÃ©es
DB_URL=sqlite:///./reviews.db
# ou pour PostgreSQL:
# DB_URL=postgresql+psycopg://user:pass@host:5432/dbname
```

### Configuration des proxies

Le scraper supporte la rotation de proxies pour Ã©viter la dÃ©tection :

```bash
# Format : http://user:pass@host:port
PROXY_POOL=http://user1:pass1@proxy1.example.com:8080,http://user2:pass2@proxy2.example.com:8080
```

## ğŸ¯ Utilisation

### Interface en ligne de commande

#### Scraping d'un ASIN unique
```bash
# Scraping basique
python -m app.cli crawl --asin B123456789

# Avec options
python -m app.cli crawl --asin B123456789 --max-pages 10 --verbose

# Avec Makefile
make run ASIN=B123456789
```

#### Scraping en lot
```bash
# Depuis un fichier
python -m app.cli crawl-batch --file scripts/examples_asins.txt --concurrency 2

# Avec Makefile
make run-batch FILE=scripts/examples_asins.txt
```

#### Export des donnÃ©es
```bash
# Export CSV
python -m app.cli export --asin B123456789 --output reviews.csv --format csv

# Export Parquet
python -m app.cli export --asin B123456789 --output reviews.parquet --format parquet

# Export de tous les avis
python -m app.cli export --output all_reviews.csv --format csv

# Avec Makefile
make export ASIN=B123456789 OUTPUT=reviews.csv
```

#### VÃ©rification du systÃ¨me
```bash
# Health check
python -m app.cli health-check

# Avec Makefile
make health
```

### Utilisation programmatique

```python
import asyncio
from app.scrape import AmazonScraper

async def main():
    scraper = AmazonScraper()
    
    # Scraping d'un ASIN
    stats = await scraper.scrape_asin("B123456789", max_pages=5)
    print(f"RÃ©cupÃ©rÃ© {stats['total_reviews']} avis")
    
    # RÃ©cupÃ©ration des avis
    reviews = scraper.get_reviews_for_asin("B123456789")
    for review in reviews:
        print(f"Rating: {review['rating']}, Auteur: {review['reviewer_name']}")

asyncio.run(main())
```

## ğŸ“Š Structure des donnÃ©es

### ModÃ¨le de donnÃ©es

Chaque avis contient les champs suivants :

| Champ | Type | Description |
|-------|------|-------------|
| `id` | Integer | ID unique auto-incrÃ©mentÃ© |
| `asin` | String(20) | ASIN du produit |
| `review_id` | String(64) | ID unique de l'avis |
| `review_title` | String(500) | Titre de l'avis |
| `review_body` | Text | Corps de l'avis |
| `rating` | Float | Note de 1.0 Ã  5.0 |
| `review_date` | String(10) | Date au format YYYY-MM-DD |
| `verified_purchase` | Boolean | Achat vÃ©rifiÃ© |
| `helpful_votes` | Integer | Nombre de votes utiles |
| `reviewer_name` | String(255) | Nom du reviewer |
| `variant` | String(255) | Variante du produit (taille, couleur) |
| `created_at` | DateTime | Date de crÃ©ation en base |
| `updated_at` | DateTime | Date de mise Ã  jour |

### Exemple de donnÃ©es exportÃ©es

```csv
asin,review_id,review_title,review_body,rating,review_date,verified_purchase,helpful_votes,reviewer_name,variant
B123456789,R123456789,"Excellent produit","TrÃ¨s satisfait de cet achat",4.5,2024-01-15,true,3,"Jean Dupont","Taille: L, Couleur: Bleu"
B123456789,R987654321,"Bon produit","Correct pour le prix",3.0,2024-01-14,false,1,"Marie Martin",
```

## ğŸ§ª Tests

### Lancement des tests

```bash
# Tests complets avec couverture
make test

# Tests rapides
make test-fast

# Tests spÃ©cifiques
pytest tests/test_normalize.py -v
```

### Couverture de code

```bash
# GÃ©nÃ©ration du rapport de couverture
pytest --cov=app --cov-report=html

# Affichage du rapport
open htmlcov/index.html
```

## ğŸ”§ DÃ©veloppement

### Configuration de l'environnement de dÃ©veloppement

```bash
# Installation complÃ¨te
make dev

# Installation des hooks pre-commit
make pre-commit

# VÃ©rifications de qualitÃ©
make ci
```

### Commandes de dÃ©veloppement

```bash
# Formatage du code
make format

# VÃ©rification du code
make lint

# VÃ©rification des types
make typecheck

# Nettoyage
make clean
```

## ğŸ³ Docker

### Construction et exÃ©cution

```bash
# Construction de l'image
make docker-build

# ExÃ©cution simple
make docker-run

# ExÃ©cution interactive
make docker-run-interactive

# Avec docker-compose (PostgreSQL inclus)
make docker-compose-up
```

### Services Docker

Le `docker-compose.yml` inclut :
- **amazon-scraper** : Service principal
- **postgres** : Base de donnÃ©es PostgreSQL
- **pgadmin** : Interface d'administration (optionnel)

```bash
# Lancement de tous les services
docker-compose up -d

# Affichage des logs
docker-compose logs -f

# ArrÃªt des services
docker-compose down
```

## ğŸ“ Structure du projet
## ğŸ”’ DÃ©duplication & intÃ©gritÃ©

- UnicitÃ© en base par `review_id` (contrainte UNIQUE).
- PrioritÃ© aux IDs Amazon (`R...`) extraits du DOM; fallback SHA1(titre+corps) si absent.
- DÃ©duplication en insertion par `review_id` ou par contenu `(asin, review_title, review_body, review_date)`.
- DÃ©duplication de secours cÃ´tÃ© UI pour lâ€™aperÃ§u et les exports.
- Commande CLI de nettoyage:

```bash
python -m app.cli dedupe --dry-run
python -m app.cli dedupe --apply
```

## â˜ï¸ DÃ©ploiement Streamlit Cloud (notes)

- Option fiable: exÃ©cuter le scraping ailleurs (VM/Render), utiliser Streamlit Cloud pour lâ€™UI uniquement (base partagÃ©e).
- Option expÃ©rimentale: tenter Playwright sur Cloud avec `apt.txt` et installation de Chromium (`python -m playwright install chromium`).

## ğŸ—‚ï¸ Versionning / fichiers ignorÃ©s

Voir `.gitignore` pour ignorer `venv/`, `reviews.db`, `pw_profile/`, `debug/`, `htmlcov/`, `streamlit.log`.

## ğŸ§ª Fixtures

Les fichiers HTML dâ€™exemple se trouvent dans `fixtures/`. Conserver un seul exemplaire UTFâ€‘8.

```
amazon-reviews-scraper/
â”œâ”€â”€ app/                    # Code source principal
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # Configuration Pydantic
â”‚   â”œâ”€â”€ db.py              # Base de donnÃ©es SQLAlchemy
â”‚   â”œâ”€â”€ models.py          # ModÃ¨les ORM
â”‚   â”œâ”€â”€ normalize.py       # Normalisation des donnÃ©es
â”‚   â”œâ”€â”€ selectors.py       # SÃ©lecteurs CSS/XPath
â”‚   â”œâ”€â”€ parser.py          # Parser des avis
â”‚   â”œâ”€â”€ fetch.py           # Gestion des requÃªtes
â”‚   â”œâ”€â”€ scrape.py          # Logique de scraping
â”‚   â”œâ”€â”€ cli.py             # Interface CLI
â”‚   â””â”€â”€ utils.py           # Utilitaires
â”œâ”€â”€ tests/                 # Tests unitaires et intÃ©gration
â”‚   â”œâ”€â”€ test_normalize.py
â”‚   â”œâ”€â”€ test_parser.py
â”‚   â”œâ”€â”€ test_utils.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ scripts/               # Scripts utilitaires
â”‚   â”œâ”€â”€ examples_asins.txt
â”‚   â””â”€â”€ init_db.sql
â”œâ”€â”€ .github/workflows/     # CI/CD
â”‚   â””â”€â”€ ci.yml
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â”œâ”€â”€ pyproject.toml        # Configuration des outils
â”œâ”€â”€ Dockerfile            # Image Docker
â”œâ”€â”€ docker-compose.yml    # Services Docker
â”œâ”€â”€ Makefile             # Commandes de dÃ©veloppement
â””â”€â”€ README.md            # Documentation
```

## ğŸš¨ Gestion des erreurs

### DÃ©tection anti-bot

Le scraper dÃ©tecte automatiquement :
- Pages CAPTCHA
- VÃ©rifications de sÃ©curitÃ©
- Trafic inhabituel
- Blocages IP

En cas de dÃ©tection, le scraper :
1. Change de proxy (si disponible)
2. Change de User-Agent
3. Attend avant de rÃ©essayer
4. Augmente progressivement le dÃ©lai

### Gestion des erreurs

```python
# Exemple de gestion d'erreurs
try:
    stats = await scraper.scrape_asin("B123456789")
    if stats["success"]:
        print(f"SuccÃ¨s: {stats['total_reviews']} avis")
    else:
        print(f"Erreurs: {stats['errors']}")
except Exception as e:
    print(f"Erreur fatale: {e}")
```

## ğŸ“ˆ Performance

### Optimisations

- **Concurrence contrÃ´lÃ©e** : Limitation du nombre de contextes simultanÃ©s
- **Rotation de proxies** : Ã‰vite la dÃ©tection et les blocages
- **Pagination intelligente** : ArrÃªt automatique si pas de page suivante
- **Cache de base de donnÃ©es** : Ã‰vite les doublons
- **Pauses alÃ©atoires** : Simule un comportement humain

### Monitoring

```bash
# VÃ©rification de l'Ã©tat
make health

# Logs dÃ©taillÃ©s
python -m app.cli crawl --asin B123456789 --verbose
```

## ğŸ¤ Contribution

### Workflow de contribution

1. Fork le repository
2. CrÃ©er une branche feature (`git checkout -b feature/amazing-feature`)
3. Commit les changements (`git commit -m 'Add amazing feature'`)
4. Push vers la branche (`git push origin feature/amazing-feature`)
5. Ouvrir une Pull Request

### Standards de code

- **Python 3.11+** avec type hints
- **Black** pour le formatage
- **Ruff** pour le linting
- **MyPy** pour la vÃ©rification des types
- **Pytest** pour les tests
- **Couverture >90%**

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸ™ Remerciements

- [Playwright](https://playwright.dev/) pour l'automatisation du navigateur
- [SQLAlchemy](https://www.sqlalchemy.org/) pour l'ORM
- [Typer](https://typer.tiangolo.com/) pour l'interface CLI
- [Pydantic](https://pydantic-docs.helpmanual.io/) pour la validation des donnÃ©es

## ğŸ“ Support

- **Issues** : [GitHub Issues](https://github.com/your-username/amazon-scraper/issues)
- **Discussions** : [GitHub Discussions](https://github.com/your-username/amazon-scraper/discussions)
- **Email** : support@amazon-scraper.com

---

**âš ï¸ Rappel** : Respectez les CGU d'Amazon et utilisez cet outil de maniÃ¨re responsable et lÃ©gale.
