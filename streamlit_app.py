"""Mini interface Streamlit pour lancer le scraping, s'authentifier et exporter."""

import asyncio
import os
from pathlib import Path
import pandas as pd
import subprocess
import streamlit as st

from app.scrape import AmazonScraper
from app.utils import setup_logging, validate_asin, parse_amazon_url, detect_login_page
from app.config import settings
from app.fetch import AmazonFetcher


def run_async(coro):
    """Ex√©cute une coroutine dans l'event loop de Streamlit."""
    return asyncio.run(coro)


st.set_page_config(page_title="Amazon Reviews Scraper", layout="wide")
st.title("üõí Amazon Reviews Scraper")
st.caption("Utilisez avec mod√©ration et respect des CGU Amazon.")

setup_logging("INFO")

# Option: installation auto de Chromium sur Streamlit Cloud si absent
try:
    import shutil
    if shutil.which("playwright") and not shutil.which("chromium"):
        # Essayer une installation silencieuse (sera ignor√©e si d√©j√† pr√©sent)
        subprocess.run(["python", "-m", "playwright", "install", "chromium", "--with-deps"], check=False)
except Exception:
    pass

tab1, tab2, tab3, tab4 = st.tabs(["Scraper", "Export", "Authentification", "Guide"]) 

with tab1:
    st.subheader("Scraper un ASIN")
    st.caption("Renseignez un ASIN ou une URL Amazon. Si l‚ÄôURL contient l‚ÄôASIN, il sera d√©tect√© automatiquement.")
    asin = st.text_input(
        "ASIN",
        placeholder="B08N5WRWNW",
        help="Identifiant produit Amazon √† 10 caract√®res alphanum√©riques (ex: B08N5WRWNW).",
    )
    url = st.text_input(
        "URL Amazon (produit ou avis)",
        placeholder="https://www.amazon.fr/.../dp/B0CJMJPXR1 ... ou ... /product-reviews/B0CJMJPXR1",
        help="Collez une URL de page produit ou directement de la page d‚Äôavis. L‚ÄôASIN et la langue seront d√©duits si possible.",
    )
    language_choice = st.selectbox(
        "Langue",
        options=["Auto (d√©duite)", "fr_FR", "en_US", "de_DE", "es_ES", "it_IT"],
        index=0,
        help="Forcer la langue des avis. Par d√©faut, d√©duite de l‚ÄôURL.",
    )
    max_pages = st.number_input(
        "Nombre de pages max",
        min_value=1,
        max_value=100,
        value=2,
        help="Limite sup√©rieure de pagination. Le scraping s‚Äôarr√™te si plus de pages ou s‚Äôil n‚Äôy a plus d‚Äôavis.",
    )
    reset_history = st.checkbox(
        "R√©initialiser l'historique pour cet ASIN",
        value=False,
        help="Supprime tous les avis existants pour cet ASIN avant le run.",
    )
    verbose = st.checkbox("Mode verbeux", value=False, help="Active des logs plus d√©taill√©s pendant le scraping.")
    start = st.button("Lancer le scraping", help="D√©marrer le scraping selon les param√®tres ci-dessus.")

    if start:
        if url:
            parsed = parse_amazon_url(url)
            if not parsed or not parsed.get("asin"):
                st.error("URL Amazon invalide. Fournissez une URL de produit ou d'avis Amazon valide.")
                st.stop()
            asin = parsed["asin"]
        if not validate_asin(asin):
            st.error("ASIN invalide. Il doit contenir exactement 10 caract√®res alphanum√©riques.")
        else:
            # Placeholders UX enrichis
            progress_bar = st.progress(0, text="Pr√™t √† d√©marrer‚Ä¶")
            stats_cols = st.columns(4)
            page_ph = stats_cols[0].empty()
            parsed_ph = stats_cols[1].empty()
            saved_ph = stats_cols[2].empty()
            duration_ph = stats_cols[3].empty()
            eta_placeholder = st.empty()
            chart_placeholder = st.empty()
            live_table_placeholder = st.empty()
            live_rows = []

            with st.status("Scraping en cours‚Ä¶", expanded=False) as status:
                scraper = AmazonScraper()
                run_started_at = pd.Timestamp.utcnow()
                domain = parsed.get("domain") if url else None
                # Priorit√© √† la langue choisie dans l'UI, sinon celle d√©duite de l'URL
                chosen_lang = None if language_choice.startswith("Auto") else language_choice
                language = chosen_lang or (parsed.get("language") if url else None)

                # Option: purge de l'historique existant pour cet ASIN
                if reset_history:
                    try:
                        deleted = scraper.delete_reviews_for_asin(asin)
                        st.info(f"Historique r√©initialis√©: {deleted} avis supprim√©s pour {asin}")
                    except Exception:
                        st.warning("R√©initialisation ignor√©e (erreur) ‚Äî le scraping continue")

                # Callback de progression (appel√© √† chaque page)
                def _progress_cb(detail: dict):
                    try:
                        live_rows.append(detail)
                        df_live = pd.DataFrame(live_rows)
                        df_live = df_live[[c for c in ["page", "reviews_parsed", "saved", "duration_s", "next", "error"] if c in df_live.columns]]
                        live_table_placeholder.dataframe(df_live, use_container_width=True)
                        current_page = int(detail.get("page", 0) or 0)
                        pct = int(min(100, round((current_page / max(1, int(max_pages))) * 100)))
                        progress_bar.progress(pct, text=f"Page {current_page}/{int(max_pages)} ‚Ä¢ {pct}%")

                        # Cartes de m√©triques
                        page_ph.metric("Page", f"{current_page}/{int(max_pages)}")
                        parsed_ph.metric("Avis pars√©s (page)", int(detail.get("reviews_parsed", 0) or 0))
                        total_saved = int(df_live.get("saved", pd.Series(dtype=int)).fillna(0).sum()) if "saved" in df_live else 0
                        saved_ph.metric("Avis enregistr√©s (cumul)", total_saved)
                        duration_ph.metric("Dur√©e page (s)", float(detail.get("duration_s", 0) or 0))

                        # ETA (estimation simple)
                        try:
                            if "duration_s" in df_live and not df_live["duration_s"].empty:
                                avg_dur = float(df_live["duration_s"].fillna(0).mean())
                                remaining = max(0, int(max_pages) - current_page)
                                eta_s = int(round(avg_dur * remaining))
                                if remaining > 0:
                                    eta_placeholder.caption(f"ETA ~ {eta_s}s restants (dur√©e moyenne {avg_dur:.1f}s/page)")
                                else:
                                    eta_placeholder.caption("Presque termin√©‚Ä¶")
                        except Exception:
                            pass

                        # Mini graphique (avis enregistr√©s par page)
                        try:
                            if "page" in df_live and "saved" in df_live:
                                df_plot = df_live[["page", "saved"]].set_index("page")
                                chart_placeholder.bar_chart(df_plot)
                        except Exception:
                            pass

                        # Mettre √† jour le statut en cours
                        try:
                            status.update(label=f"Traitement page {current_page}‚Ä¶", state="running")
                        except Exception:
                            pass
                    except Exception:
                        pass

                stats = run_async(scraper.scrape_asin(
                    asin,
                    max_pages=int(max_pages),
                    domain=domain,
                    language=language,
                    progress_cb=_progress_cb,
                ))
            # Mettre √† jour le statut final selon le r√©sultat
            try:
                if stats.get("success"):
                    # Fermer proprement la barre en la remplissant √† 100%
                    progress_bar.progress(100, text="Termin√© ‚Ä¢ 100%")
                else:
                    progress_bar.progress(min(100, progress_bar._value or 0), text="Termin√© avec erreurs")
            except Exception:
                pass
            # Affichage r√©capitulatif clair pour utilisateur novice
            st.markdown("### R√©sultat")
            status_msg_ok = (
                f"‚úÖ Termin√©: {stats.get('total_reviews',0)} avis sauvegard√©s sur {stats.get('total_pages',0)} pages"
                f" ‚Ä¢ {stats.get('total_encountered',0)} rencontr√©s"
                f" ‚Ä¢ {stats.get('total_duplicates',0)} doublons"
            )
            status_msg_warn = f"‚ö† Termin√© avec erreurs: {len(stats.get('errors', []))} erreurs"
            if stats.get("success"):
                st.success(status_msg_ok)
            else:
                st.warning(status_msg_warn)

            # D√©tails par page
            pages_details = stats.get("pages_details") or []
            if pages_details:
                df_pages = pd.DataFrame(pages_details)
                df_pages = df_pages[[c for c in ["page", "reviews_parsed", "saved", "duration_s", "next", "error"] if c in df_pages.columns]]
                st.markdown("#### D√©tail par page")
                st.dataframe(df_pages, use_container_width=True)

                # Indicateur pagination compl√®te
                try:
                    last_next = bool(df_pages.iloc[-1]["next"]) if not df_pages.empty and "next" in df_pages.columns else None
                    total_pages = int(stats.get("total_pages") or len(df_pages))
                    asked_pages = int(max_pages)
                    is_complete = (total_pages == asked_pages) or (last_next is False)
                    st.caption(f"Pagination: {total_pages}/{asked_pages} pages ‚Ä¢ " + ("‚úÖ compl√®te" if is_complete else "‚ö† incompl√®te"))
                except Exception:
                    pass

                # Export du d√©tail de pagination
                csv_pages = df_pages.to_csv(index=False).encode("utf-8")
                st.download_button("Exporter le d√©tail de pagination (CSV)", data=csv_pages, file_name=f"pages_details_{asin}.csv", mime="text/csv")

            # (section fusionn√©e ci-dessous)

            # R√©sultats & aper√ßu (fusion: derniers 10 + aper√ßu tri√©)
            try:
                # Priorit√© aux avis ins√©r√©s pendant ce run
                try:
                    recent_inserted = scraper.get_reviews_for_asin_since(asin, created_after=run_started_at.to_pydatetime())
                except Exception:
                    recent_inserted = []
                if recent_inserted:
                    all_reviews = pd.DataFrame(recent_inserted)
                else:
                    all_reviews = pd.DataFrame(scraper.get_reviews_for_asin(asin))
                if not all_reviews.empty:
                    # D√©duplication de secours c√¥t√© UI pour garantir z√©ro doublon visible
                    dedupe_cols = [c for c in ["asin", "review_title", "review_body", "review_date"] if c in all_reviews.columns]
                    if dedupe_cols:
                        all_reviews = all_reviews.drop_duplicates(subset=dedupe_cols, keep="first")
                    def _sentiment_from_rating(r: float) -> str:
                        try:
                            val = float(r)
                        except Exception:
                            return "Neutre"
                        if val >= 4:
                            return "Positif"
                        if val <= 2:
                            return "N√©gatif"
                        return "Neutre"
                    st.markdown("#### R√©sultats et aper√ßu")
                    tab_last, tab_preview = st.tabs(["Chronologique", "Aper√ßu tri√© par sentiment"])

                    # Vue 1: Derniers 10 avis (par date de cr√©ation si dispo, sinon review_date)
                    with tab_last:
                        sort_cols = [c for c in ["created_at", "review_date"] if c in all_reviews.columns]
                        df_last = all_reviews.copy()
                        if sort_cols:
                            df_last = df_last.sort_values(sort_cols, ascending=[False] * len(sort_cols))
                        cols_last = [c for c in ["review_id", "review_date", "rating", "review_title", "review_body", "reviewer_name", "variant"] if c in df_last.columns]
                        st.dataframe(df_last[cols_last], use_container_width=True)
                        # Exports de la vue chronologique compl√®te
                        c1, c2 = st.columns(2)
                        with c1:
                            st.download_button(
                                "CSV (chronologique)",
                                data=df_last.to_csv(index=False).encode("utf-8"),
                                file_name=f"reviews_chrono_{asin}.csv",
                                mime="text/csv",
                                key=f"dl_chrono_csv_{asin}",
                            )
                        with c2:
                            try:
                                import io
                                out = io.BytesIO()
                                with pd.ExcelWriter(out, engine="openpyxl") as writer:
                                    df_last.to_excel(writer, index=False, sheet_name="chronologique")
                                st.download_button(
                                    "Excel (chronologique)",
                                    data=out.getvalue(),
                                    file_name=f"reviews_chrono_{asin}.xlsx",
                                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                    key=f"dl_chrono_xlsx_{asin}",
                                )
                            except Exception:
                                pass

                    # Vue 2: Aper√ßu tri√© par sentiment
                    with tab_preview:
                        all_reviews["sentiment"] = all_reviews.get("rating", 0).apply(_sentiment_from_rating)
                        order = ["Positif", "Neutre", "N√©gatif"]
                        all_reviews["sentiment"] = pd.Categorical(all_reviews["sentiment"], categories=order, ordered=True)
                        df_preview = all_reviews.sort_values(["sentiment", "review_date"], ascending=[True, False])
                        st.dataframe(df_preview, use_container_width=True)

                        col1, col2 = st.columns(2)
                        with col1:
                            csv_bytes = df_preview.to_csv(index=False).encode("utf-8")
                            st.download_button(
                                "CSV (aper√ßu tri√©)",
                                data=csv_bytes,
                                file_name=f"reviews_{asin}.csv",
                                mime="text/csv",
                                key=f"dl_csv_sorted_{asin}",
                            )
                        with col2:
                            try:
                                import io
                                output = io.BytesIO()
                                with pd.ExcelWriter(output, engine="openpyxl") as writer:
                                    df_preview.to_excel(writer, index=False, sheet_name="reviews")
                                st.download_button(
                                    "Excel (aper√ßu tri√©)",
                                    data=output.getvalue(),
                                    file_name=f"reviews_{asin}.xlsx",
                                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                    key=f"dl_xlsx_sorted_{asin}",
                                )
                            except Exception:
                                pass
            except Exception:
                pass

            # Afficher erreurs si pr√©sentes
            if stats.get("errors"):
                with st.expander("Voir les erreurs"):
                    for err in stats["errors"]:
                        st.write(f"- {err}")

with tab2:
    st.subheader("Exporter les avis")
    st.caption("T√©l√©chargez les avis d√©j√† stock√©s en base. Laissez l‚ÄôASIN vide pour exporter tout le jeu de donn√©es.")
    # Conserver l'√©tat entre les reruns pour √©viter la disparition des boutons apr√®s clic
    if "export_df" not in st.session_state:
        st.session_state["export_df"] = None
    asin_filter = st.text_input(
        "Filtrer par ASIN (optionnel)",
        help="Exporter uniquement les avis correspondant √† cet ASIN. Laissez vide pour tout exporter.",
    )
    limit = st.number_input(
        "Limiter le nombre d'avis (optionnel)",
        min_value=0,
        max_value=10000,
        value=0,
        help="0 pour aucune limite. Utile pour √©chantillonner les donn√©es.",
    )
    do_export = st.button("Pr√©parer l'export", help="Pr√©parer les fichiers t√©l√©chargeables ci-dessous.")

    if do_export:
        scraper = AmazonScraper()
        lim = int(limit) if limit > 0 else None
        data = scraper.get_reviews_for_asin(asin_filter, lim) if asin_filter else scraper.get_all_reviews(lim)
        if not data:
            st.session_state["export_df"] = None
            st.info("Aucun avis en base.")
        else:
            st.session_state["export_df"] = pd.DataFrame(data)

    # Proposer les deux formats d'export tant que des donn√©es sont pr√™tes
    if st.session_state.get("export_df") is not None:
        df = st.session_state["export_df"]
        col_a, col_b = st.columns(2)
        with col_a:
            st.download_button(
                "T√©l√©charger CSV",
                data=df.to_csv(index=False).encode("utf-8"),
                file_name="reviews.csv",
                mime="text/csv",
                key="dl_csv_global",
            )
        with col_b:
            try:
                import io
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine="openpyxl") as writer:
                    df.to_excel(writer, index=False, sheet_name="reviews")
                st.download_button(
                    "T√©l√©charger Excel",
                    data=output.getvalue(),
                    file_name="reviews.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key="dl_xlsx_global",
                )
            except Exception:
                st.warning("Excel indisponible: v√©rifiez l'installation d'openpyxl.")

with tab3:
    st.subheader("Authentification Amazon")
    st.caption("Ouvrez une fen√™tre pour vous connecter, la session sera enregistr√©e.")
    with st.expander("Comment √ßa marche ?"):
        st.markdown(
            "- Une fen√™tre Amazon s‚Äôouvre avec Playwright.\n"
            "- Connectez-vous normalement (email, mot de passe, 2FA si n√©cessaire).\n"
            "- √Ä la d√©tection de session valide, l‚Äô√©tat est sauvegard√© pour les prochains scrapings."
        )

    session_state_path = getattr(settings, "storage_state_path", "./storage_state.json")
    if os.path.exists(session_state_path):
        st.success(f"Session trouv√©e: {session_state_path}")
    else:
        st.info("Aucune session enregistr√©e.")

    if st.button("Ouvrir la fen√™tre de connexion Amazon", help="Lancer une fen√™tre contr√¥l√©e pour se connecter √† Amazon."):
        with st.spinner("Ouverture de la fen√™tre de connexion..."):
            old = settings.headless
            settings.headless = False
            async def run_login(timeout_sec: int = 600) -> bool:
                fetcher = AmazonFetcher()
                await fetcher.start_browser()
                context = await fetcher.create_context()
                page = await context.new_page()
                # Accueil FR puis navigation via header
                await page.goto("https://www.amazon.fr/", wait_until="domcontentloaded", timeout=settings.timeout_ms)
                try:
                    for sel in [
                        'input#sp-cc-accept',
                        'input[data-cel-widget="sp-cc-accept"]',
                        'input[name="accept"]',
                    ]:
                        btn = await page.query_selector(sel)
                        if btn:
                            await btn.click()
                            break
                except Exception:
                    pass
                try:
                    acc = await page.wait_for_selector('#nav-link-accountList', timeout=8000)
                    await acc.click()
                    await page.wait_for_load_state("domcontentloaded")
                except Exception:
                    signin_url = (
                        "https://www.amazon.fr/ap/signin?_encoding=UTF8"
                        "&openid.assoc_handle=frflex"
                        "&openid.return_to=https%3A%2F%2Fwww.amazon.fr%2F%3Fref_%3Dnav_signin"
                        "&openid.mode=checkid_setup&ignoreAuthState=1"
                        "&openid.ns=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0"
                        "&openid.claimed_id=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select"
                        "&openid.identity=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select"
                    )
                    await page.goto(signin_url, wait_until="domcontentloaded", timeout=settings.timeout_ms)
                import time as _t
                start = _t.time()
                logged = False
                while _t.time() - start < timeout_sec:
                    try:
                        content = await page.content()
                        if not detect_login_page(content):
                            acc = await page.query_selector('#nav-link-accountList')
                            if acc:
                                txt = (await acc.inner_text()) or ""
                                if "Identifiez-vous" not in txt:
                                    logged = True
                                    break
                        await page.wait_for_timeout(1500)
                    except Exception:
                        await page.wait_for_timeout(1500)
                        continue
                if logged:
                    await context.storage_state(path=session_state_path)
                await fetcher.stop_browser()
                return logged
            ok = asyncio.run(run_login())
            settings.headless = old
            if ok:
                st.success(f"‚úì Session enregistr√©e: {session_state_path}")
            else:
                st.error("Connexion non d√©tect√©e dans le d√©lai imparti.")

with tab4:
    st.subheader("Guide d‚Äôutilisation")
    st.markdown("""
### 1) Scraper un produit
- Renseignez un **ASIN** ou collez une **URL Amazon** (produit ou avis). Si l‚ÄôURL contient l‚ÄôASIN, il sera d√©tect√©.
- Choisissez la **langue** (ou laissez en Auto). R√©glez le **nombre de pages max**.
- Cliquez sur **Lancer le scraping**. Suivez la **progression** et le **d√©tail par page**.

### 2) Exporter les avis
- Allez dans l‚Äôonglet **Export**. Filtrez par **ASIN** si besoin, sinon laissez vide.
- Choisissez le **format** (CSV ou Parquet) puis cliquez sur **Exporter**.

### 3) Authentification Amazon
- Ouvrez une fen√™tre de connexion depuis l‚Äôonglet **Authentification**.
- Connectez-vous; la **session** est enregistr√©e pour les scrapings suivants.

### Conseils et bonnes pratiques
- Respectez les **CGU Amazon** et limitez la fr√©quence (pauses min/max dans la barre lat√©rale).
- Pour une meilleure stabilit√©, commencez avec **peu de pages** puis augmentez.
- Surveillez la section **erreurs** en fin de scraping. Relancez si besoin.

### D√©pannage
- Si aucune donn√©e n‚Äôappara√Æt: v√©rifiez l‚ÄôASIN/URL et la **langue**.
- Si Amazon demande un captcha/2FA: passez par l‚Äôonglet **Authentification** puis r√©essayez.
- Erreurs r√©seau sporadiques: **relancez** le scraping, r√©duisez **pages max**, augmentez les **pauses**.
    """)

st.sidebar.header("Configuration rapide")
st.sidebar.caption("Param√®tres appliqu√©s au processus courant. Utiles pour ajuster le rythme et la stabilit√©.")
headless = st.sidebar.checkbox("Headless (recommand√©)", value=True, help="Ex√©cute le navigateur sans interface graphique pour plus de performances.")
pages_env = st.sidebar.number_input("Pages max par d√©faut", min_value=1, max_value=100, value=5, help="Valeur utilis√©e si vous ne pr√©cisez pas de limite dans l‚Äôonglet Scraper.")
sleep_min = st.sidebar.number_input("Pause min (s)", min_value=0.0, value=2.0, help="D√©lai minimum al√©atoire entre les pages.")
sleep_max = st.sidebar.number_input("Pause max (s)", min_value=0.0, value=4.0, help="D√©lai maximum al√©atoire entre les pages.")

if st.sidebar.button("Appliquer", help="Enregistrer ces param√®tres dans les variables d‚Äôenvironnement du processus courant."):
    # Applique √† l'environnement courant du process (simple)
    os.environ["HEADLESS"] = "true" if headless else "false"
    os.environ["MAX_PAGES_PER_ASIN"] = str(int(pages_env))
    os.environ["SLEEP_MIN"] = str(float(sleep_min))
    os.environ["SLEEP_MAX"] = str(float(sleep_max))
    st.sidebar.success("Configuration appliqu√©e (process courant)")

st.sidebar.markdown("---")
st.sidebar.write("Pour lancer:")
st.sidebar.code("streamlit run streamlit_app.py")


